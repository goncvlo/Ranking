{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fa35c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import optuna\n",
    "\n",
    "from src.data.load import load_data\n",
    "from src.data.prepare import prepare_data\n",
    "from src.models.co_visit import CoVisit\n",
    "from src.models.baseline import popular_items\n",
    "from src.features.features import feature_engineering\n",
    "from src.features.utils import build_rank_input\n",
    "from src.models.tuner import BayesianSearch\n",
    "from src.models.ranker import Ranker\n",
    "from src.models.evaluator import Evaluation\n",
    "from src.models.tracker import launch_mlflow, Logging\n",
    "from src.models.utils import leave_last_k, set_global_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cbdb26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read config\n",
    "with open('config.yml', 'r') as file:\n",
    "    config=yaml.load(file, Loader= yaml.SafeLoader)\n",
    "del file\n",
    "\n",
    "# ensure reproducibility\n",
    "set_global_seed(seed=config[\"general\"][\"seed\"])\n",
    "\n",
    "# set experiment tracking\n",
    "launch_mlflow()\n",
    "\n",
    "# set params\n",
    "ALGORITHM = \"XGBRanker\"\n",
    "neg_sample_k= 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0569496d",
   "metadata": {},
   "source": [
    "**Data Preparation & Train/Test Split**\n",
    "\n",
    "- Load and transform the 3 datasets\n",
    "- Split whole set into train, validation and test sets by segmenting it temporally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de3ceb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare data\n",
    "dfs = load_data(config=config['data_loader'])\n",
    "dfs = prepare_data(dataframes=dfs, config=config[\"data_preparation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbe7e5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "df_train, df_test = leave_last_k(df=dfs['data'], config=config['optimization'])\n",
    "df_train, df_valid = leave_last_k(df=df_train, config=config['optimization'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eb620d",
   "metadata": {},
   "source": [
    "**Feature Engineering & Optimization**\n",
    "\n",
    "- Add negative samples, i.e., items likely to be disliked by user\n",
    "- Feature Engineering - creates cross user-item features for ranking model\n",
    "- Search which hyper-parameters optimize scoring metric for the given algorithm in the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa845bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative samples for different sources\n",
    "neg_sample_1 = popular_items(ui_matrix=df_train, k=neg_sample_k)\n",
    "neg_sample_2 = CoVisit(methods=[\"negative\"]).fit(ui_matrix=df_train)\n",
    "neg_sample = pd.concat([neg_sample_1, neg_sample_2], ignore_index=True)\n",
    "\n",
    "neg_sample = neg_sample[[\"user_id\", \"item_id\"]]\n",
    "neg_sample[\"rating\"] = list(config[\"data_preparation\"][\"rating_conversion\"].keys())[0]\n",
    "\n",
    "del neg_sample_1, neg_sample_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d85b542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build features for ranking model\n",
    "user_item_features = feature_engineering(\n",
    "    dataframes={'user': dfs['user'], 'item': dfs['item'], 'data': df_train}\n",
    "    )\n",
    "\n",
    "# add negative samples and merge features\n",
    "df_train = pd.concat([df_train, neg_sample], ignore_index=True)\n",
    "df_train, df_valid = [\n",
    "    build_rank_input(ratings=df.iloc[:,:3], features=user_item_features)\n",
    "    for df in (df_train, df_valid)\n",
    "    ]\n",
    "\n",
    "del neg_sample, user_item_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc2fdc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 19:10:43,758] A new study created in memory with name: no-name-8d045930-2e36-4e26-96a8-261f9a0b7d7c\n",
      "[I 2025-07-25 19:10:49,719] Trial 0 finished with value: 0.9475062672322376 and parameters: {'learning_rate': 0.1878955193048339, 'gamma': 9.50714306409916, 'max_depth': 12, 'subsample': 0.7993292420985183, 'n_estimators': 104}. Best is trial 0 with value: 0.9475062672322376.\n",
      "[I 2025-07-25 19:11:11,276] Trial 1 finished with value: 0.9495919300106045 and parameters: {'learning_rate': 0.07884126564776513, 'gamma': 0.5808361216819946, 'max_depth': 14, 'subsample': 0.8005575058716043, 'n_estimators': 298}. Best is trial 1 with value: 0.9495919300106045.\n",
      "[I 2025-07-25 19:11:17,560] Trial 2 finished with value: 0.9478975821845176 and parameters: {'learning_rate': 0.011271662653605422, 'gamma': 9.699098521619943, 'max_depth': 13, 'subsample': 0.6061695553391381, 'n_estimators': 113}. Best is trial 1 with value: 0.9495919300106045.\n",
      "[I 2025-07-25 19:11:25,002] Trial 3 finished with value: 0.9485241463414635 and parameters: {'learning_rate': 0.09251885041686347, 'gamma': 3.0424224295953772, 'max_depth': 9, 'subsample': 0.7159725093210578, 'n_estimators': 152}. Best is trial 1 with value: 0.9495919300106045.\n",
      "[I 2025-07-25 19:11:33,932] Trial 4 finished with value: 0.9469068080593849 and parameters: {'learning_rate': 0.30631459446646736, 'gamma': 1.3949386065204183, 'max_depth': 6, 'subsample': 0.6831809216468459, 'n_estimators': 210}. Best is trial 1 with value: 0.9495919300106045.\n",
      "[I 2025-07-25 19:11:38,680] Trial 5 finished with value: 0.9492201908801697 and parameters: {'learning_rate': 0.3928028047351138, 'gamma': 1.9967378215835974, 'max_depth': 9, 'subsample': 0.7962072844310213, 'n_estimators': 66}. Best is trial 1 with value: 0.9495919300106045.\n",
      "[I 2025-07-25 19:11:51,236] Trial 6 finished with value: 0.9476459384941676 and parameters: {'learning_rate': 0.30416488109881773, 'gamma': 1.7052412368729153, 'max_depth': 3, 'subsample': 0.9744427686266666, 'n_estimators': 388}. Best is trial 1 with value: 0.9495919300106045.\n",
      "[I 2025-07-25 19:11:59,192] Trial 7 finished with value: 0.9468378791092259 and parameters: {'learning_rate': 0.4043902767101141, 'gamma': 3.0461376917337066, 'max_depth': 4, 'subsample': 0.8421165132560784, 'n_estimators': 204}. Best is trial 1 with value: 0.9495919300106045.\n",
      "[I 2025-07-25 19:12:06,180] Trial 8 finished with value: 0.9484309968186638 and parameters: {'learning_rate': 0.06189707918754463, 'gamma': 4.951769101112702, 'max_depth': 3, 'subsample': 0.954660201039391, 'n_estimators': 140}. Best is trial 1 with value: 0.9495919300106045.\n",
      "[I 2025-07-25 19:12:12,520] Trial 9 finished with value: 0.948899830328738 and parameters: {'learning_rate': 0.331598619892637, 'gamma': 3.1171107608941098, 'max_depth': 9, 'subsample': 0.7733551396716398, 'n_estimators': 114}. Best is trial 1 with value: 0.9495919300106045.\n"
     ]
    }
   ],
   "source": [
    "# set tuner for hyperparam optimization\n",
    "tuner = BayesianSearch(\n",
    "    config[\"optimization\"],\n",
    "    method=\"ranker\",\n",
    "    algorithm=ALGORITHM\n",
    "    )\n",
    "\n",
    "def objective(trial) -> float:\n",
    "    return tuner.fit(df_train, df_valid, trial)\n",
    "\n",
    "# set study\n",
    "study = optuna.create_study(\n",
    "    direction=config[\"optimization\"][\"ranker\"][\"direction\"],\n",
    "    sampler=optuna.samplers.TPESampler(seed=config[\"general\"][\"seed\"])\n",
    "    )\n",
    "study.optimize(objective, n_trials= config[\"optimization\"][\"n_trials\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec5619e",
   "metadata": {},
   "source": [
    "**Evaluation & Logging**\n",
    "\n",
    "- Recover full training set to evaluate results on test set\n",
    "- Apply negative sampling and perform feature engineering\n",
    "- Log run's information through mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9039a41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get anti test-set, i.e., train & validation sets together\n",
    "df_train = dfs[\"data\"].merge(\n",
    "    df_test\n",
    "    , on=df_test.columns.to_list(), how=\"left\"\n",
    "    , indicator=True\n",
    "    )\n",
    "df_train = df_train[df_train[\"_merge\"] == \"left_only\"].drop(columns=[\"_merge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dae45ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative samples for different sources\n",
    "neg_sample_1 = popular_items(ui_matrix=df_train, k=neg_sample_k)\n",
    "neg_sample_2 = CoVisit(methods=[\"negative\"]).fit(ui_matrix=df_train)\n",
    "neg_sample = pd.concat([neg_sample_1, neg_sample_2], ignore_index=True)\n",
    "\n",
    "neg_sample = neg_sample[[\"user_id\", \"item_id\"]]\n",
    "neg_sample[\"rating\"] = list(config[\"data_preparation\"][\"rating_conversion\"].keys())[0]\n",
    "\n",
    "del neg_sample_1, neg_sample_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72d10d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features for ranking model\n",
    "user_item_features = feature_engineering(\n",
    "    dataframes={'user': dfs['user'], 'item': dfs['item'], 'data': df_train}\n",
    "    )\n",
    "\n",
    "# add negative samples and merge features\n",
    "df_train = pd.concat([df_train, neg_sample], ignore_index=True)\n",
    "df_train, df_test = [\n",
    "    build_rank_input(ratings=df.iloc[:,:3], features=user_item_features)\n",
    "    for df in (df_train, df_test)\n",
    "    ]\n",
    "\n",
    "del neg_sample, user_item_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "771bf1c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.991994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.951817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ndcg\n",
       "dataset          \n",
       "train    0.991994\n",
       "test     0.951817"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set algorithm best hyperparams\n",
    "hyperparams = (\n",
    "    config[\"optimization\"][\"ranker\"][ALGORITHM][\"fixed\"]\n",
    "    | study.best_trial.params\n",
    ")\n",
    "\n",
    "# fit model on whole training set\n",
    "clf = Ranker(algorithm=ALGORITHM, params=hyperparams)\n",
    "clf.fit(X=df_train[\"X\"], y=df_train[\"y\"], group=df_train[\"group\"])\n",
    "tuner.artifacts[\"models\"][-1] = clf\n",
    "\n",
    "# test set evaluation\n",
    "scorer = Evaluation(clf=clf)\n",
    "tuner.artifacts[\"metrics_test\"] = scorer.fit(train=tuple(df_train.values()), test=tuple(df_test.values()))\n",
    "tuner.artifacts[\"metrics_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68035a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99653773cb6744d29c31946c2ef066dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdabb6c43a2f4d139cb4c5b3ae58353b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run 25JUL2025_191241 at: http://127.0.0.1:5000/#/experiments/878870372315907999/runs/87f1f23bcb684e9791faaf34b2e3b249\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/878870372315907999\n",
      "ðŸƒ View run XGBRanker at: http://127.0.0.1:5000/#/experiments/878870372315907999/runs/3d1abfb0857f453daf475b5a16050721\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/878870372315907999\n"
     ]
    }
   ],
   "source": [
    "# logging experiment\n",
    "logging = Logging(\n",
    "    experiment_name=\"Ranker\",\n",
    "    run_name=ALGORITHM,\n",
    "    input_sample=df_train[\"X\"].head()\n",
    "    )\n",
    "logging.log_run(study=study, tuner=tuner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbc255b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shouldn't be done with test set\n",
    "# recs_score(df_test. iloc[:, :2], df_train.iloc[:, :3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
